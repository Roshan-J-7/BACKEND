# Vision Model Module

CLIP-based lightweight medical image analysis for visual similarity matching (non-diagnostic).

## ğŸ¯ Purpose

Provide structured visual analysis of medical images for:
- Skin conditions
- Wounds/injuries  
- Medical photos
- Kiosk/telemedicine applications

**âœ… AWS Free Tier Friendly** - Only ~400MB download, ~500-700MB RAM usage

**âš ï¸ IMPORTANT: Does NOT diagnose. Only matches visual features.**

---

## ğŸ“ Module Structure

```
app/vision_model/
â”œâ”€â”€ __init__.py           # Module initialization
â”œâ”€â”€ vision_config.py      # CLIP config + 40 medical descriptors
â”œâ”€â”€ vision_client.py      # Model loader & similarity matching
â”œâ”€â”€ vision_routes.py      # FastAPI endpoints
â””â”€â”€ README.md            # This file
```

---

## ğŸš€ Installation

### 1. Install Dependencies

```bash
pip install torch torchvision transformers pillow python-multipart
```

**For CPU-only (recommended):**
```bash
pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu
pip install transformers pillow python-multipart
```

### 2. First Run (Downloads Model)

The model downloads automatically on first use:

- **Model:** `openai/clip-vit-base-patch32`
- **Size:** ~400 MB (much lighter than BLIP-2!)
- **Cache Location:** `~/.cache/huggingface/`  
- **Download Time:** 1-3 minutes

**ğŸš¨ DO NOT COMMIT THE MODEL TO GIT!** (Already in `.gitignore`)

---

## ğŸ”Œ API Endpoints

### 1. **Analyze Image (Predefined Descriptors)**

```bash
curl -X POST "http://localhost:8000/vision/analyze" \
  -F "file=@rash.jpg" \
  -F "top_k=5"
```

**Response:**
```json
{
  "top_matches": [
    {
      "descriptor_key": "circular_rash",
      "descriptor_text": "circular or ring-shaped rash",
      "confidence": 0.42
    },
    {
      "descriptor_key": "redness",
      "descriptor_text": "red inflamed skin",
      "confidence": 0.38
    },
    {
      "descriptor_key": "fungal_infection",
      "descriptor_text": "fungal skin infection appearance",
      "confidence": 0.21
    }
  ],
  "categorized_matches": {
    "pattern": [...],
    "color": [...],
    "conditions": [...]
  },
  "total_descriptors_checked": 40,
  "model_info": {
    "model": "openai/clip-vit-base-patch32",
    "device": "cpu",
    "confidence_threshold": 0.15
  }
}
```

---

### 2. **Analyze with Custom Labels**

```bash
curl -X POST "http://localhost:8000/vision/analyze-custom" \
  -F "file=@image.jpg" \
  -F "labels=red circular rash,fungal infection,normal skin,burn injury" \
  -F "top_k=3"
```

**Returns:** Top 3 matching labels from your custom list

---

### 3. **Get Available Descriptors**

```bash
curl http://localhost:8000/vision/descriptors
```

**Response:**
```json
{
  "total_descriptors": 40,
  "categories": {
    "color": 5,
    "texture": 5,
    "pattern": 5,
    "wounds": 5,
    "conditions": 5,
    "inflammation": 3,
    "borders": 3,
    "severity": 3,
    "baseline": 2,
    "features": 4
  },
  "sample_descriptors": {
    "redness": "red inflamed skin",
    "pale_skin": "pale or whitish skin",
    "darkened_skin": "darkened or hyperpigmented skin",
    ...
  }
}
```

---

### 4. **Health Check**

```bash
curl http://localhost:8000/vision/health
```

**Response:**
```json
{
  "status": "healthy",
  "model_loaded": true,
  "model_name": "openai/clip-vit-base-patch32",
  "device": "cpu",
  "total_descriptors": 40
}
```

---

## âš™ï¸ Configuration

Edit `vision_config.py` or set environment variables:

### Environment Variables

```env
# Device (cpu or cuda)
VISION_DEVICE=cpu

# Load model on startup (true/false)
VISION_LOAD_ON_STARTUP=true

# Custom cache directory (optional)
HF_CACHE_DIR=/path/to/cache
```

### Model Settings

```python
# Change confidence threshold
VISION_CONFIDENCE_THRESHOLD = 0.15  # Only return matches > 15%

# Change number of top matches
VISION_MAX_MATCHES = 5
```

---

## ğŸŒ Deployment to AWS Free Tier

### âœ… Why CLIP Works on Free Tier

| Resource | CLIP ViT-B/32 | BLIP-2 (Comparison) |
|----------|---------------|---------------------|
| Model Download | ~400 MB | 3-5 GB âŒ |
| RAM Usage | ~500-700 MB | 2-4 GB âŒ |
| Inference Speed | 1-3 sec | 30-60 sec âŒ |
| CPU Friendly | âœ… Yes | âš ï¸ Slow |
| t2.micro Safe | âœ… Yes | âŒ OOM crash |

---

### Deployment Strategy

**Option 1: Download on First Run** (Recommended)

1. Deploy code to AWS (GitHub â†’ EC2/Elastic Beanstalk)
2. Set `VISION_LOAD_ON_STARTUP=true`
3. First startup downloads model (~2-3 min)
4. Subsequent startups instant (model cached)

**Option 2: Docker with Pre-Download**

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Pre-download CLIP model
RUN python -c "from transformers import CLIPProcessor, CLIPModel; \
    CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32'); \
    CLIPModel.from_pretrained('openai/clip-vit-base-patch32')"

COPY . .

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**Docker image:** ~1.5 GB (vs 5-6 GB with BLIP-2)

---

## ğŸ“Š How CLIP Works

### Architecture

```
Image â†’ CLIP Encoder â†’ Embedding Vector
                           â†“
Text Descriptors â†’ CLIP Encoder â†’ Embedding Vectors
                           â†“
                    Cosine Similarity
                           â†“
                  Confidence Scores (0-1)
```

### Example

```python
from PIL import Image
from app.vision_model.vision_client import vision_client

# Load model
vision_client.load_model()

# Analyze
image = Image.open("rash.jpg")
result = vision_client.analyze_image(image, top_k=5)

for match in result["top_matches"]:
    print(f"{match['descriptor_text']}: {match['confidence']:.2f}")
```

**Output:**
```
circular or ring-shaped rash: 0.42
red inflamed skin: 0.38
fungal skin infection appearance: 0.21
bumpy or raised skin surface: 0.18
dry flaky peeling skin: 0.16
```

---

## ğŸ¯ Medical Descriptors (40 Total)

### Categories:

1. **Color** (5): redness, pale, darkened, yellowing, bluish
2. **Texture** (5): smooth, rough, bumpy, blistered, dry/flaky
3. **Pattern** (5): circular, scattered, linear, widespread, localized
4. **Wounds** (5): open wound, closed wound, bruising, burn, abrasion
5. **Conditions** (5): fungal, bacterial, allergic, eczema, psoriasis
6. **Inflammation** (3): swollen, inflammation, no swelling
7. **Borders** (3): well-defined, irregular, raised
8. **Severity** (3): mild, moderate, severe
9. **Baseline** (2): normal skin, healing skin
10. **Features** (4): crusted, weeping, pigmentation change, hair loss

**See full list in `vision_config.py`**

---

## ğŸ† Judge Positioning

When asked why you chose CLIP over larger models:

> "We use a lightweight vision encoder (CLIP) for deterministic, low-latency inference on resource-constrained hardware. This ensures reliability in rural/remote deployments and maintains energy efficiency. Heavy generative vision models were avoided to prevent OOM crashes on edge devices and free-tier cloud instances while still providing structured, medical-relevant visual analysis."

**This sounds:**
- âœ… Technically mature
- âœ… Deployment-aware
- âœ… Practical for real-world constraints

---

## ğŸ”’ Medical Safety

### What CLIP Does

âœ… Identifies visual similarity to predefined descriptors  
âœ… Returns structured confidence scores  
âœ… Fast, deterministic, explainable  

### What CLIP Does NOT Do

âŒ Diagnose medical conditions  
âŒ Generate free-form medical text  
âŒ Provide treatment recommendations  

**All descriptors are non-diagnostic visual features.**

---

## ğŸ› Troubleshooting

### Model Download Fails

```bash
# Manual download
python -c "from transformers import CLIPProcessor, CLIPModel; \
    CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32'); \
    CLIPModel.from_pretrained('openai/clip-vit-base-patch32')"
```

### Out of Memory

**Problem:** RAM usage too high  
**Solution:**
- Reduce batch sizes (use single image analysis)
- Use smaller image resolution
- Close other applications

### Slow Inference

**Current:** 1-3 sec on CPU  
**If too slow:**
- Use GPU (`VISION_DEVICE=cuda`)
- Reduce image size before upload
- Use batch analysis for multiple images

---

## ğŸ“š References

- [CLIP Paper](https://arxiv.org/abs/2103.00020)
- [OpenAI CLIP Model Card](https://huggingface.co/openai/clip-vit-base-patch32)
- [Transformers Documentation](https://huggingface.co/docs/transformers)

---

## ğŸ”„ Next Steps

1. âœ… Test locally with sample medical images
2. âœ… Adjust descriptor list for your use case
3. âœ… Deploy to AWS free tier (t2.micro works!)
4. âœ… Integrate with Cerebras LLM for enhanced explanations
5. âœ… Add image preprocessing pipeline
6. âœ… Implement result caching
